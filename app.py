import streamlit as st
import feedparser
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dateutil import parser
import sqlite3
import requests
from fpdf import FPDF
import re

# ===============================
# 1. CONFIGURATION & SOURCES
# ===============================

NEWS_SOURCES = [
    {'id': 'enab', 'name': 'Enab Baladi', 'url': 'https://english.enabbaladi.net/feed/'},
    {'id': 'zaman', 'name': 'Zaman Al Wasl', 'url': 'https://en.zamanalwsl.net/rss.php'},
    {'id': 'direct', 'name': 'Syria Direct', 'url': 'https://syriadirect.org/feed/'},
    {'id': 'halab', 'name': 'Halab Today', 'url': 'https://halabtodaytv.net/feed'},
    {'id': 'npa', 'name': 'North Press', 'url': 'https://npasyria.com/en/feed/'},
    {'id': 'hawar', 'name': 'Hawar News', 'url': 'https://hawarnews.com/en/feed/'},
    {'id': 'rojava', 'name': 'Rojava Info', 'url': 'https://rojavainformationcenter.org/feed/'},
    {'id': 'sana', 'name': 'SANA (Gov)', 'url': 'https://sana.sy/en/?feed=rss2'},
    {'id': 'suwayda', 'name': 'Suwayda 24', 'url': 'https://suwayda24.com/feed/'},
    {'id': 'deir', 'name': 'DeirEzzor 24', 'url': 'https://deirezzor24.net/en/feed/'},
    {'id': 'observer', 'name': 'The Syrian Observer', 'url': 'https://syrianobserver.com/feed'},
]

TOPIC_KEYWORDS = {
    'Humanitarian': ['aid', 'refugee', 'camp', 'food', 'water', 'cholera', 'earthquake', 'unrwa', 'displacement', 'shelter', 'poverty', 'starvation'],
    'Military/Ground': ['shelling', 'clash', 'airstrike', 'air strike', 'bombing', 'killed', 'injured', 'attack', 'isis', 'islamic state', 'ied', 'drone', 'assassination', 'frontline', 'front line'],
    'Political': ['meeting', 'decree', 'election', 'minister', 'normalization', 'astana', 'geneva', 'un sc', 'security council', 'diplomacy', 'statement', 'agreement', 'talks', 'negotiation'],
    'Human Rights': ['arrest', 'torture', 'detainee', 'prison', 'detention', 'kidnap', 'kidnapping', 'execution', 'violation', 'forced', 'activist', 'enforced disappearance'],
}

ACTOR_KEYWORDS = {
    'Regime/SAA': ['assad', 'regime', 'saa', 'syrian army', 'government forces', 'damascus', '4th division'],
    'SDF/Kurdish': ['sdf', 'kurdish', 'ypg', 'asayish', 'aanes', 'mazloum'],
    'Turkey/SNA': ['turkey', 'turkish', 'sna', 'national army', 'ankara', 'mercenaries'],
    'HTS/Idlib': ['hts', 'hayat tahrir', 'jolani', 'salvation government', 'idlib'],
    'Russia': ['russia', 'russian', 'moscow', 'putin', 'khmeimim'],
    'Iran/Militias': ['iran', 'tehran', 'militia', 'irgc', 'hezbollah'],
    'USA/Coalition': ['usa', 'american', 'coalition', 'washington', 'base', 'pentagon'],
    'Israel': ['israel', 'idf', 'tel aviv', 'golani brigade'],
}

DB_PATH = "syria_monitor.db"

# ===============================
# 2. ROBUST PDF GENERATION
# ===============================

class SitRepPDF(FPDF):
    def __init__(self):
        # Explicitly set A4 and margins immediately to avoid geometry errors
        super().__init__(orientation='P', unit='mm', format='A4')
        self.set_margins(10, 10, 10) 
        self.set_auto_page_break(auto=True, margin=15)
        # Explicit effective width: 210mm (A4) - 10mm (Left) - 10mm (Right) = 190mm
        self.epw_safe = 190 

    def header(self):
        self.set_font('Helvetica', 'B', 14)
        # Use explicit width
        self.cell(self.epw_safe, 8, 'Syria Conflict Monitor | Political Affairs Briefing', ln=True, align='C')
        self.ln(2)
        self.set_draw_color(0, 0, 0)
        self.line(10, 20, 200, 20)
        self.ln(6)

    def footer(self):
        self.set_y(-15)
        self.set_font('Helvetica', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}/{{nb}} - Generated by Syria Watch Pulse', align='C')

    def add_section_title(self, label):
        self.ln(2)
        self.set_font('Helvetica', 'B', 11)
        self.set_fill_color(240, 240, 240)
        # Explicit width prevents "no horizontal space" error
        self.cell(self.epw_safe, 8, f"  {label}", fill=True, ln=True, align='L')
        self.ln(2)

    def add_article(self, title, source, text):
        # Title Block
        self.set_font('Helvetica', 'B', 10)
        self.multi_cell(self.epw_safe, 5, f"{title} [{source}]", align='L')
        
        # Text Block
        self.set_font('Helvetica', '', 9)
        self.multi_cell(self.epw_safe, 5, text, align='L')
        
        # Separator line
        self.ln(2)
        current_y = self.get_y()
        self.set_draw_color(220, 220, 220)
        self.line(10, current_y, 200, current_y)
        self.ln(3)
        self.set_draw_color(0, 0, 0)

def clean_text_for_pdf(text):
    """
    Ruthlessly sanitizes text to prevent FPDF layout crashes.
    Removes non-breaking spaces, newlines, and converts to Latin-1.
    """
    if not text: return ""
    
    # 1. Replace problematic whitespace (The #1 cause of the error)
    # \xa0 is non-breaking space. It prevents wrapping, causing the error.
    text = text.replace('\xa0', ' ').replace('\t', ' ').replace('\n', ' ').replace('\r', '')
    
    # 2. Normalize quotes
    replacements = {
        '\u2018': "'", '\u2019': "'", '\u201c': '"', '\u201d': '"',
        '\u2013': '-', '\u2014': '-', '\u2026': '...'
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    
    # 3. Collapse multiple spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 4. Transliterate/Ignore non-Latin characters (Arabic/Cyrillic will disappear or become ?)
    # Standard FPDF cannot render Arabic. This ensures the PDF generates successfully.
    return text.encode('latin-1', 'replace').decode('latin-1')

def generate_sitrep_pdf(df):
    """
    Generates a prioritized 2-page PDF.
    Priority: Political > Ground > Humanitarian.
    """
    if df.empty: return None

    # 1. Setup Buckets & Priorities
    df_sorted = df.sort_values(by='relevance_score', ascending=False).copy()
    
    # Limit total characters to ~6000 (approx 2 dense pages)
    CHAR_LIMIT = 6000
    current_chars = 0
    used_links = set()
    
    final_content = {
        "Political Developments": [],
        "Situation on the Ground": [],
        "Humanitarian and Human Rights": []
    }

    def process_item(row):
        full = row['full_text'] if row['full_text'] and len(str(row['full_text'])) > 200 else row['summary']
        if not full: full = "No details available."
        
        # Clean immediately
        clean_body = clean_text_for_pdf(full)
        clean_title = clean_text_for_pdf(row['title'])
        clean_source = clean_text_for_pdf(row['source'])
        
        # Cap length for density (max 600 chars per article)
        if len(clean_body) > 600:
            clean_body = clean_body[:600].rsplit(' ', 1)[0] + "..."
            
        return clean_title, clean_source, clean_body

    # 2. The "Anchor" Phase: Ensure Top 1 from each category is present
    cats = {
        "Political Developments": "Political",
        "Situation on the Ground": "Military/Ground",
        "Humanitarian and Human Rights": "Humanitarian|Human Rights"
    }
    
    # Temporary helper to find top item
    for section, keyword in cats.items():
        matches = df_sorted[df_sorted['tags'].astype(str).str.contains(keyword, regex=True)]
        if not matches.empty:
            row = matches.iloc[0]
            if row['link'] not in used_links:
                t, s, b = process_item(row)
                final_content[section].append((t, s, b))
                used_links.add(row['link'])
                current_chars += len(b) + len(t) + 50

    # 3. The "Fill" Phase: Prioritize Political, then others
    for index, row in df_sorted.iterrows():
        if current_chars >= CHAR_LIMIT: break
        if row['link'] in used_links: continue
        
        tags = str(row['tags'])
        target_section = None
        
        if "Political" in tags: target_section = "Political Developments"
        elif "Military/Ground" in tags: target_section = "Situation on the Ground"
        elif "Humanitarian" in tags or "Human Rights" in tags: target_section = "Humanitarian and Human Rights"
        
        if target_section:
            # Weighting Logic:
            # Always add Political.
            # Add Ground/Humanitarian only if we have space AND Political section has > 1 item
            allow_add = False
            if target_section == "Political Developments":
                allow_add = True
            elif len(final_content["Political Developments"]) >= 2:
                allow_add = True
            
            if allow_add:
                t, s, b = process_item(row)
                final_content[target_section].append((t, s, b))
                used_links.add(row['link'])
                current_chars += len(b) + len(t) + 50

    # 4. Render PDF
    pdf = SitRepPDF()
    pdf.alias_nb_pages()
    pdf.add_page()
    
    pdf.set_font('Helvetica', '', 9)
    pdf.cell(0, 5, f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')} | Articles Processed: {len(df)}", ln=True)
    
    order = ["Political Developments", "Situation on the Ground", "Humanitarian and Human Rights"]
    
    for section in order:
        items = final_content[section]
        # Always show section header even if empty, to show structure
        pdf.add_section_title(section)
        
        if not items:
            pdf.set_font('Helvetica', 'I', 9)
            pdf.cell(190, 6, "No significant high-priority updates in this window.", ln=True)
        else:
            for (title, source, body) in items:
                pdf.add_article(title, source, body)
                
    return pdf.output(dest='S').encode('latin-1')

# ===============================
# 3. DATABASE & SCRAPING UTILS
# ===============================

def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS articles (link TEXT PRIMARY KEY, title TEXT, source TEXT, published_date TIMESTAMP, summary TEXT, full_text TEXT, tags TEXT, actors TEXT, relevance_score REAL, red_flags TEXT, fetched_at TIMESTAMP)''')
    # Migration check
    c.execute("PRAGMA table_info(articles)")
    cols = [r[1] for r in c.fetchall()]
    for col in ["full_text", "relevance_score", "red_flags"]:
        if col not in cols:
            c.execute(f"ALTER TABLE articles ADD COLUMN {col} TEXT") # Simplified type
    conn.commit()
    conn.close()

def fetch_full_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            # Try article tag first, else all paragraphs
            container = soup.find('article') or soup
            paras = container.find_all('p')
            return "\n\n".join([p.get_text(" ", strip=True) for p in paras])
    except:
        return ""
    return ""

def analyze_content(title, text, source):
    txt = (title + " " + text).lower()
    
    # Tags
    tags = []
    for k, v in TOPIC_KEYWORDS.items():
        if any(w in txt for w in v): tags.append(k)
    if not tags: tags.append("General")
    
    # Actors
    actors = []
    for k, v in ACTOR_KEYWORDS.items():
        if any(w in txt for w in v): actors.append(k)
        
    # Relevance
    score = 1.0
    if "Political" in tags: score += 1.5
    if "Military/Ground" in tags: score += 1.0
    if any(x in txt for x in ["president", "minister", "summit", "un sc"]): score += 2.0
    if any(x in txt for x in ["killed", "massacre"]): score += 1.0
    
    return ", ".join(tags), ", ".join(actors), min(5.0, score)

def ingest_feeds(lookback_hrs):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    cutoff = datetime.now() - timedelta(hours=lookback_hrs)
    count = 0
    
    for s in NEWS_SOURCES:
        try:
            f = feedparser.parse(s['url'])
            for e in f.entries:
                try:
                    dt = parser.parse(e.get('published', e.get('updated', str(datetime.now())))).replace(tzinfo=None)
                except: dt = datetime.now()
                
                if dt < cutoff: continue
                link = e.get('link', '')
                if not link: continue
                
                # Check DB
                c.execute("SELECT link FROM articles WHERE link=?", (link,))
                if c.fetchone(): continue
                
                # Fetch
                full = fetch_full_text(link)
                summary = BeautifulSoup(e.get('summary', ''), 'html.parser').get_text()
                title = e.get('title', 'No Title')
                
                tags, actors, score = analyze_content(title, full or summary, s['name'])
                
                c.execute("INSERT INTO articles VALUES (?,?,?,?,?,?,?,?,?,?,?)", 
                          (link, title, s['name'], dt, summary, full, tags, actors, score, "None", datetime.now()))
                count += 1
        except Exception as e:
            print(f"Failed {s['name']}: {e}")
            
    conn.commit()
    conn.close()
    return count

def load_articles(lookback_hrs):
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query("SELECT * FROM articles WHERE published_date >= ? ORDER BY published_date DESC", 
                           conn, params=(datetime.now() - timedelta(hours=lookback_hrs),))
    conn.close()
    return df

# ===============================
# 4. STREAMLIT UI
# ===============================

st.set_page_config(page_title="Syria Monitor", layout="wide")
init_db()

st.title("üá∏üáæ Syria Conflict News Monitor")
st.markdown("Political Affairs Office | Automated SitRep Generator")

with st.sidebar:
    st.header("Controls")
    lookback = st.slider("Lookback (Hours)", 24, 168, 72, 24)
    if st.button("üîÑ Refresh Data"):
        with st.spinner("Scraping sources... this may take a minute..."):
            n = ingest_feeds(lookback)
        st.success(f"Ingested {n} new articles.")

# Load data once
df = load_articles(lookback)

tab1, tab2, tab3 = st.tabs(["üìä Dashboard", "üìù PDF Briefing", "üóÑ Data View"])

with tab1:
    if not df.empty:
        c1, c2, c3 = st.columns(3)
        c1.metric("Total Articles", len(df))
        c2.metric("Political", len(df[df['tags'].str.contains('Political')]))
        c3.metric("Military", len(df[df['tags'].str.contains('Military')]))
        
        st.subheader("Latest High Priority Items")
        st.dataframe(
            df[df['relevance_score'] >= 3.0][['published_date', 'title', 'source', 'tags', 'relevance_score']],
            use_container_width=True,
            hide_index=True
        )
    else:
        st.info("No data found. Please click 'Refresh Data' in the sidebar.")

with tab2:
    st.subheader("Generate 2-Page SitRep")
    st.markdown("""
    This tool generates a PDF Situation Report prioritized for the Political Affairs Office:
    1. **Political Developments** (Highest Priority)
    2. **Situation on the Ground**
    3. **Humanitarian Updates**
    
    *Note: The generator ensures the document fits ~2 pages.*
    """)
    
    if not df.empty:
        if st.button("üìÑ Create PDF SitRep"):
            with st.spinner("Generating PDF..."):
                try:
                    pdf_bytes = generate_sitrep_pdf(df)
                    if pdf_bytes:
                        st.session_state['sitrep_pdf'] = pdf_bytes
                        st.success("PDF Generated Successfully!")
                    else:
                        st.warning("Not enough data to generate report.")
                except Exception as e:
                    st.error(f"Error: {e}")
        
        if 'sitrep_pdf' in st.session_state:
            st.download_button(
                "‚¨áÔ∏è Download SitRep PDF",
                data=st.session_state['sitrep_pdf'],
                file_name=f"Syria_SitRep_{datetime.now().date()}.pdf",
                mime="application/pdf"
            )
    else:
        st.warning("No data available to generate PDF.")

with tab3:
    st.subheader("Raw Data")
    if not df.empty:
        st.dataframe(df)
