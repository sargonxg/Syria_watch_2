import streamlit as st
import feedparser
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dateutil import parser
import sqlite3
import requests
from fpdf import FPDF
import re

# ===============================
# 1. CONFIGURATION & SOURCES
# ===============================

NEWS_SOURCES = [
    {'id': 'enab', 'name': 'Enab Baladi', 'url': 'https://english.enabbaladi.net/feed/'},
    {'id': 'zaman', 'name': 'Zaman Al Wasl', 'url': 'https://en.zamanalwsl.net/rss.php'},
    {'id': 'direct', 'name': 'Syria Direct', 'url': 'https://syriadirect.org/feed/'},
    {'id': 'halab', 'name': 'Halab Today', 'url': 'https://halabtodaytv.net/feed'},
    {'id': 'npa', 'name': 'North Press', 'url': 'https://npasyria.com/en/feed/'},
    {'id': 'hawar', 'name': 'Hawar News', 'url': 'https://hawarnews.com/en/feed/'},
    {'id': 'rojava', 'name': 'Rojava Info', 'url': 'https://rojavainformationcenter.org/feed/'},
    {'id': 'sana', 'name': 'SANA (Gov)', 'url': 'https://sana.sy/en/?feed=rss2'},
    {'id': 'suwayda', 'name': 'Suwayda 24', 'url': 'https://suwayda24.com/feed/'},
    {'id': 'deir', 'name': 'DeirEzzor 24', 'url': 'https://deirezzor24.net/en/feed/'},
    {'id': 'observer', 'name': 'The Syrian Observer', 'url': 'https://syrianobserver.com/feed'},
]

TOPIC_KEYWORDS = {
    'Humanitarian': ['aid', 'refugee', 'camp', 'food', 'water', 'cholera', 'earthquake', 'unrwa', 'displacement', 'shelter', 'poverty', 'starvation'],
    'Military/Ground': ['shelling', 'clash', 'airstrike', 'air strike', 'bombing', 'killed', 'injured', 'attack', 'isis', 'islamic state', 'ied', 'drone', 'assassination', 'frontline', 'front line'],
    'Political': ['meeting', 'decree', 'election', 'minister', 'normalization', 'astana', 'geneva', 'un sc', 'security council', 'diplomacy', 'statement', 'agreement', 'talks', 'negotiation'],
    'Human Rights': ['arrest', 'torture', 'detainee', 'prison', 'detention', 'kidnap', 'kidnapping', 'execution', 'violation', 'forced', 'activist', 'enforced disappearance'],
}

ACTOR_KEYWORDS = {
    'Regime/SAA': ['assad', 'regime', 'saa', 'syrian army', 'government forces', 'damascus', '4th division'],
    'SDF/Kurdish': ['sdf', 'kurdish', 'ypg', 'asayish', 'aanes', 'mazloum'],
    'Turkey/SNA': ['turkey', 'turkish', 'sna', 'national army', 'ankara', 'mercenaries'],
    'HTS/Idlib': ['hts', 'hayat tahrir', 'jolani', 'salvation government', 'idlib'],
    'Russia': ['russia', 'russian', 'moscow', 'putin', 'khmeimim'],
    'Iran/Militias': ['iran', 'tehran', 'militia', 'irgc', 'hezbollah'],
    'USA/Coalition': ['usa', 'american', 'coalition', 'washington', 'base', 'pentagon'],
    'Israel': ['israel', 'idf', 'tel aviv', 'golani brigade'],
}

DB_PATH = "syria_monitor.db"

# ===============================
# 2. ROBUST PDF GENERATION
# ===============================

class SitRepPDF(FPDF):
    def __init__(self):
        # Explicitly set A4 (210x297mm) and margins
        super().__init__(orientation='P', unit='mm', format='A4')
        self.set_margins(10, 10, 10) 
        self.set_auto_page_break(auto=True, margin=15)
        # Safe width: 210 - 10 (left) - 10 (right) = 190
        self.epw_safe = 190 

    def header(self):
        self.set_font('Helvetica', 'B', 14)
        self.cell(self.epw_safe, 8, 'Syria Conflict Monitor | Political Affairs Briefing', ln=True, align='C')
        self.ln(2)
        self.set_draw_color(0, 0, 0)
        self.line(10, 20, 200, 20)
        self.ln(6)

    def footer(self):
        self.set_y(-15)
        self.set_font('Helvetica', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}/{{nb}} - Generated by Syria Watch Pulse', align='C')

    def add_section_title(self, label):
        self.ln(2)
        self.set_font('Helvetica', 'B', 11)
        self.set_fill_color(240, 240, 240)
        self.cell(self.epw_safe, 8, f"  {label}", fill=True, ln=True, align='L')
        self.ln(2)

    def add_article(self, title, source, text):
        self.set_font('Helvetica', 'B', 10)
        self.multi_cell(self.epw_safe, 5, f"{title} [{source}]", align='L')
        
        self.set_font('Helvetica', '', 9)
        self.multi_cell(self.epw_safe, 5, text, align='L')
        
        self.ln(2)
        y = self.get_y()
        self.set_draw_color(220, 220, 220)
        self.line(10, y, 200, y)
        self.ln(3)
        self.set_draw_color(0, 0, 0)

def clean_text_for_pdf(text):
    """Sanitizes text to prevent FPDF encoding/layout errors."""
    if not text: return ""
    
    # Remove non-breaking spaces and tabs (Crucial for layout safety)
    text = text.replace('\xa0', ' ').replace('\t', ' ').replace('\n', ' ').replace('\r', '')
    
    # Standardize quotes/dashes
    replacements = {
        '\u2018': "'", '\u2019': "'", '\u201c': '"', '\u201d': '"',
        '\u2013': '-', '\u2014': '-', '\u2026': '...'
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    
    # Collapse spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Force Latin-1 (replace unsupported chars with ?)
    return text.encode('latin-1', 'replace').decode('latin-1')

def generate_sitrep_pdf(df):
    """Generates a 2-page PDF with strict prioritization."""
    if df.empty: return None

    # Sort by relevance
    df_sorted = df.sort_values(by='relevance_score', ascending=False).copy()
    
    # Character budget for ~2 pages
    CHAR_LIMIT = 5500
    current_chars = 0
    used_links = set()
    
    content = {
        "Political Developments": [],
        "Situation on the Ground": [],
        "Humanitarian and Human Rights": []
    }

    def format_article(row):
        # Use full text if available, else summary
        raw = row['full_text'] if row['full_text'] and len(str(row['full_text'])) > 200 else row['summary']
        if not raw: raw = "No details available."
        
        clean_b = clean_text_for_pdf(raw)
        clean_t = clean_text_for_pdf(row['title'])
        clean_s = clean_text_for_pdf(row['source'])
        
        # Density Cap: 550 chars per article max
        if len(clean_b) > 550:
            clean_b = clean_b[:550].rsplit(' ', 1)[0] + "..."
            
        return clean_t, clean_s, clean_b

    # --- PHASE 1: ANCHORS (Top 1 per section) ---
    keywords = {
        "Political Developments": "Political",
        "Situation on the Ground": "Military/Ground",
        "Humanitarian and Human Rights": "Humanitarian|Human Rights"
    }
    
    for section, regex in keywords.items():
        match = df_sorted[df_sorted['tags'].astype(str).str.contains(regex, regex=True)]
        if not match.empty:
            row = match.iloc[0]
            if row['link'] not in used_links:
                t, s, b = format_article(row)
                content[section].append((t, s, b))
                used_links.add(row['link'])
                current_chars += len(b) + len(t) + 50

    # --- PHASE 2: FILL (Prioritize Political) ---
    for _, row in df_sorted.iterrows():
        if current_chars >= CHAR_LIMIT: break
        if row['link'] in used_links: continue
        
        tags = str(row['tags'])
        target = None
        if "Political" in tags: target = "Political Developments"
        elif "Military/Ground" in tags: target = "Situation on the Ground"
        elif "Humanitarian" in tags or "Human Rights" in tags: target = "Humanitarian and Human Rights"
        
        if target:
            # Logic: Always add Political. Add others only if Political has enough depth (>=2 items)
            allow = False
            if target == "Political Developments":
                allow = True
            elif len(content["Political Developments"]) >= 2:
                allow = True
            
            if allow:
                t, s, b = format_article(row)
                content[target].append((t, s, b))
                used_links.add(row['link'])
                current_chars += len(b) + len(t) + 50

    # --- RENDER ---
    pdf = SitRepPDF()
    pdf.alias_nb_pages()
    pdf.add_page()
    
    pdf.set_font('Helvetica', '', 9)
    pdf.cell(0, 5, f"Date: {datetime.now().strftime('%d %B %Y')} | Articles Analyzed: {len(df)}", ln=True)
    
    for section in ["Political Developments", "Situation on the Ground", "Humanitarian and Human Rights"]:
        pdf.add_section_title(section)
        items = content[section]
        if not items:
            pdf.set_font('Helvetica', 'I', 9)
            pdf.cell(190, 6, "No high-priority events detected.", ln=True)
        else:
            for t, s, b in items:
                pdf.add_article(t, s, b)
                
    # Return bytes directly (Fixes 'encode' error)
    return bytes(pdf.output())

# ===============================
# 3. DATABASE & SCRAPING
# ===============================

def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS articles (link TEXT PRIMARY KEY, title TEXT, source TEXT, published_date TIMESTAMP, summary TEXT, full_text TEXT, tags TEXT, actors TEXT, relevance_score REAL, red_flags TEXT, fetched_at TIMESTAMP)''')
    c.execute("PRAGMA table_info(articles)")
    cols = [r[1] for r in c.fetchall()]
    for col in ["full_text", "relevance_score", "red_flags"]:
        if col not in cols: c.execute(f"ALTER TABLE articles ADD COLUMN {col} TEXT")
    conn.commit()
    conn.close()

def fetch_text(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            container = soup.find('article') or soup
            return "\n\n".join([p.get_text(" ", strip=True) for p in container.find_all('p')])
    except: return ""
    return ""

def analyze(title, text):
    txt = (title + " " + text).lower()
    tags = [k for k, v in TOPIC_KEYWORDS.items() if any(w in txt for w in v)]
    if not tags: tags.append("General")
    
    actors = [k for k, v in ACTOR_KEYWORDS.items() if any(w in txt for w in v)]
    
    score = 1.0
    if "Political" in tags: score += 1.5
    if "Military/Ground" in tags: score += 1.0
    if any(x in txt for x in ["president", "un sc", "summit", "killed"]): score += 1.5
    
    return ", ".join(tags), ", ".join(actors), min(5.0, score)

def ingest(hours):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    cutoff = datetime.now() - timedelta(hours=hours)
    count = 0
    for s in NEWS_SOURCES:
        try:
            f = feedparser.parse(s['url'])
            for e in f.entries:
                dt = parser.parse(e.get('published', e.get('updated', str(datetime.now())))).replace(tzinfo=None)
                if dt < cutoff: continue
                
                link = e.get('link')
                if not link: continue
                
                c.execute("SELECT link FROM articles WHERE link=?", (link,))
                if c.fetchone(): continue
                
                full = fetch_text(link)
                summ = BeautifulSoup(e.get('summary', ''), 'html.parser').get_text()
                title = e.get('title', 'No Title')
                tags, acts, score = analyze(title, full or summ)
                
                c.execute("INSERT INTO articles VALUES (?,?,?,?,?,?,?,?,?,?,?)", 
                          (link, title, s['name'], dt, summ, full, tags, acts, score, "None", datetime.now()))
                count += 1
        except: pass
    conn.commit()
    conn.close()
    return count

def load(hours):
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query("SELECT * FROM articles WHERE published_date >= ? ORDER BY published_date DESC", conn, params=(datetime.now() - timedelta(hours=hours),))
    conn.close()
    return df

# ===============================
# 4. UI
# ===============================

st.set_page_config(page_title="Syria Monitor", layout="wide")
init_db()

st.title("üá∏üáæ Syria Conflict News Monitor")
st.markdown("**Political Affairs Office | Automated SitRep Generator**")

with st.sidebar:
    lookback = st.slider("Window (Hours)", 24, 168, 72, 24)
    if st.button("üîÑ Refresh Data"):
        with st.spinner("Scraping..."):
            n = ingest(lookback)
        st.success(f"Ingested {n} items.")

df = load(lookback)

tab1, tab2 = st.tabs(["üìù PDF Briefing", "üìä Dashboard"])

with tab1:
    st.subheader("Generate 2-Page SitRep")
    if not df.empty:
        if st.button("üìÑ Generate PDF"):
            with st.spinner("Generating..."):
                try:
                    pdf_data = generate_sitrep_pdf(df)
                    st.session_state['pdf'] = pdf_data
                    st.success("Done!")
                except Exception as e: st.error(f"Error: {e}")
        
        if 'pdf' in st.session_state:
            st.download_button("‚¨áÔ∏è Download PDF", st.session_state['pdf'], "SitRep.pdf", "application/pdf")
    else: st.warning("No data.")

with tab2:
    if not df.empty:
        st.dataframe(df[['published_date','source','tags','relevance_score','title']], use_container_width=True)
